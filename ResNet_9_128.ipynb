{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaZZI8wb-9np"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVU8TgqX_1oW"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    '''\n",
        "    This residual block is defined by He et al.\n",
        "\n",
        "    @article{He2015,\n",
        "\t  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n",
        "   \ttitle = {Deep Residual Learning for Image Recognition},\n",
        "  \tjournal = {arXiv preprint arXiv:1512.03385},\n",
        "\t  year = {2015}\n",
        "    }\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_res1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
        "                                   kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
        "        self.conv_res1_bn = nn.BatchNorm2d(num_features=out_channels, momentum=0.9)\n",
        "        self.conv_res2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
        "                                   kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.conv_res2_bn = nn.BatchNorm2d(num_features=out_channels, momentum=0.9)\n",
        "\n",
        "        if stride != 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(num_features=out_channels, momentum=0.9))\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.relu(self.conv_res1_bn(self.conv_res1(x)))\n",
        "        out = self.conv_res2_bn(self.conv_res2(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(residual)\n",
        "\n",
        "        out = self.relu(out)\n",
        "        out = out + residual\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_YQdMPcARCX"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    # This is the 9 layer residual network.\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            ResidualBlock(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            \n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            ResidualBlock(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(-1, out.shape[1] * out.shape[2] * out.shape[3])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XjOkAadAnwK"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = Net().to(\"cpu\")\n",
        "summary(model, (3,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmAXea9MBCFM"
      },
      "outputs": [],
      "source": [
        "# Starting with a batch size of 128\n",
        "batch_size = 128\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
        "\n",
        "# Using the CIFAR-10 Dataset\n",
        "\n",
        "training_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "train_load = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "test_load = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g61D4tLxCIEL"
      },
      "outputs": [],
      "source": [
        "for X, Y in train_load:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  print(X[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvAXSJxrCudR"
      },
      "outputs": [],
      "source": [
        "# using Adam optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b5RZsb2CzIJ"
      },
      "outputs": [],
      "source": [
        "train_acc = []\n",
        "test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-JqGFdLC4aA"
      },
      "outputs": [],
      "source": [
        "def train(data, model, loss_fn, optimizer):\n",
        "  size = len(data.dataset)\n",
        "  model.train()\n",
        "  count = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(data):\n",
        "    X = X.to(\"cpu\")\n",
        "    y = y.to(\"cpu\")\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()  \n",
        "    optimizer.step()\n",
        "\n",
        "    count += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    if batch % 100 == 0:\n",
        "      loss = loss.item()\n",
        "      current = len(X) * batch\n",
        "\n",
        "      print(f\"Data Loss: {loss:>6f} [{current:>5d}/{size:>5d}]\")\n",
        "  \n",
        "  train_acc.append(count/size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIMCVdAZDO05"
      },
      "outputs": [],
      "source": [
        "def test(data, model, loss_fn):\n",
        "  size = len(data.dataset)\n",
        "  num_batches = len(data)\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  count = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for X, y in data:\n",
        "      X = X.to(\"cpu\")\n",
        "      y = y.to(\"cpu\")\n",
        "\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "      count += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        " \n",
        "  test_loss /= num_batches\n",
        "  count /= size\n",
        "  test_acc.append(count)\n",
        "  print(f\"Accuracy: {(100*count):>0.01f}%, Average Loss: {test_loss:>7f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKO9r8aeDiLh"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n\")\n",
        "  train(train_load, model, loss_fn, optimizer)\n",
        "  test(test_load, model, loss_fn)\n",
        "  print(\"\\n---------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIZ4O6knEFiV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = [i for i in range(epochs)]\n",
        "plt.plot(x,train_acc, label = 'train_acc')\n",
        "plt.plot(x,test_acc, label = 'test_acc')\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Batch Size: 128')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}